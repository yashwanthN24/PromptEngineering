# PromptEngineering

![image](https://github.com/user-attachments/assets/ea87be4a-587c-48ea-975d-5661a454b180)

To test out the api the LLMS provide a playground to test out the apis 

![image](https://github.com/user-attachments/assets/ad68f46d-bda6-4b68-997f-7b747ec5e3f6)

![image](https://github.com/user-attachments/assets/18ae70ac-a4af-4df9-8a86-f1cb42062f32)

![image](https://github.com/user-attachments/assets/136ea63b-e82b-40cf-8ea4-0a2235abd1c7)

use : Either Temperature or Top P 
      Either Maximum length or stop sequence
      Either Frequency penalty or pressence penalty 

Top p or Temperature is jist for making sure the response is creative and is not repatative i.e increasing randomness and not relying on just facts 

these are setting u can set via interacting with the LLM as an API 

![image](https://github.com/user-attachments/assets/df041fd1-ef22-4584-b10c-9b22eb49c163)

Here system is the persona like who are you what role are you in (like i am a software engineer) 

User is the response that has to be returned for that prompt in system 

Assistent is the response you want the model to give back 

![image](https://github.com/user-attachments/assets/bc17e904-30dd-4d40-9680-c2ee19eadabe)

So when giving prompt these are the four Prompt Elements to keep in mind 

Instruction : What u want to do 

Context : for whom or onbehalf of whom what role (software engineer or doctor etc) 

Input date : The input you want to give 

output Indicator or Format : Json , CSV , excel etc 





